// Copyright (c) 2025 John Doe
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//	http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
package aillm

import (
	"context"
	"errors"
	"fmt"
	"log"
	"os"
	"strconv"

	"github.com/redis/go-redis/v9"

	"github.com/tmc/langchaingo/llms"
	"github.com/tmc/langchaingo/schema"
)

// Init initializes the LLMContainer by setting up memory management, embedding configuration,
// transcriber settings, and connecting to the Redis database.
//
// This function configures various components of the LLMContainer, such as memory management,
// text embedding, and Redis connectivity.
//
// Returns:
//   - error: An error if Redis host is not configured or the connection fails.
func (llm *LLMContainer) Init() error {
	var err error

	// Default Semantic search algorithm
	if llm.SearchAlgorithm == 0 {
		llm.SearchAlgorithm = SimilaritySearch
	}

	// Initialize memory management with a capacity of 300 entries

	llm.MemoryManager = NewMemoryManager(300)
	// Configure text embedding parameters with chunking settings

	ec := EmbeddingConfig{
		ChunkSize:    2048, // Size of each text chunk
		ChunkOverlap: 100,  // Overlap between consecutive chunks for context retention
	}

	llm.EmbeddingConfig = ec

	// Retrieve Tika service URL from environment variables for text processing

	llm.Transcriber.TikaURL = os.Getenv("TikaURL")
	if llm.Transcriber.TikaURL == "" {
		log.Println("Warning: Tika host configuration is missing. As a result, the transcriber will be restricted to processing only text and HTML files.")

	}
	// Initialize the transcriber component

	llm.Transcriber.init()
	// Load Redis configuration from environment variables if not already set

	if llm.DataRedis.Host == "" {
		llm.DataRedis.Host = os.Getenv("REDIS_HOST")
		llm.DataRedis.Password = os.Getenv("REDIS_PASSWORD")
	}
	// Check if Redis host is configured, return an error if missing

	if llm.DataRedis.Host == "" {
		return errors.New("missing redis host configuration")
	}

	// Establish a connection to the Redis server
	llm.DataRedis.redisClient = redis.NewClient(&redis.Options{
		Addr:     llm.DataRedis.Host,     // آدرس Redis
		Password: llm.DataRedis.Password, // آدرس Redis
		DB:       0,                      // شماره دیتابیس
	})
	ctx := context.Background()
	// Test Redis connection
	_, err = llm.DataRedis.redisClient.Ping(ctx).Result()
	if err != nil {
		return fmt.Errorf("unable to connect to redis host. \n%v", err)
	}
	// predefine basic values
	if llm.Temperature == 0 {
		llm.Temperature = 0.01
	}
	if llm.TopP == 0 {
		llm.TopP = 0.01
	}

	if llm.ScoreThreshold == 0 {
		llm.ScoreThreshold = 0.75
	}

	if llm.RagRowCount == 0 {
		llm.RagRowCount = 5
	}

	llm.AnswerLanguage = "English"

	if llm.NoRagErrorMessage == "" {
		llm.NoRagErrorMessage = "You have to say sadly I don't have any data."
	}

	if llm.NotRelatedAnswer == "" {
		llm.NotRelatedAnswer = "I can't find any answer regarding your question."
	}


	return err
}

// AskLLM processes a user query and retrieves an AI-generated response, leveraging memory and RAG (Retrieval-Augmented Generation).
//
// The function checks for available memory, retrieves contextually relevant documents from the embedding store,
// and formats the query to be processed by the LLM. If no relevant documents are found and hallucination is not allowed,
// the function returns an error.
//
// Parameters:
//   - prefix: A string prefix used for query-related operations.
//   - Language: The language in which the response should be generated.
//   - sessionID: Unique identifier for the user session to track history and context.
//   - Query: The user's input query for which an answer is requested.
//   - StreamingFunc: A callback function to handle streaming output from the LLM.
//
// Returns:
//   - *llms.ContentResponse: The response generated by the LLM.
//   - error: An error if there is a failure in processing.
func (llm *LLMContainer) AskLLM(prefix, Language, sessionID, Query string, StreamingFunc func(ctx context.Context, chunk []byte) error) (*llms.ContentResponse,interface{}, error) {
	// Retrieve memory for the session

	mem, exists := llm.MemoryManager.GetMemory(sessionID)

	ctx := context.Background()
	// Check if LLM client is available

	if llm.LLMClient == nil {
		return nil,nil, errors.New("missing llm client")
	}
	// Check if embedding model is available

	if llm.Embedder == nil {
		return nil,nil, errors.New("missing embedding model")
	} else {
		// Initialize embedding model if not already initialized

		if !llm.Embedder.initialized() {
			llm.InitEmbedding()
		}
	}
	// Initialize the LLM client for processing

	llmclient, err := llm.LLMClient.NewLLMClient()
	if err != nil {
		return nil,nil, err
	}
	var msgs []llms.MessageContent
	// Add AI assistant's character/personality setting
	if llm.Character != "" {
		msgs = append(msgs, llms.TextParts(llms.ChatMessageTypeSystem, llm.Character))
	}
	// Construct the query prefix for the embedding store
	KNNPrefix := prefix + ":"
	if Language == "" {
		if llm.FallbackLanguage != "" {
			Language = llm.FallbackLanguage
		} else {
			return nil,nil, errors.New("missing language. you can use fallback language if you want to use backup contents")
		}
	}
	KNNPrefix += Language + ":"
	KNNQuery := Query

	// Append past session queries to provide context

	for _, memoryItem := range mem.Questions {
		KNNQuery += "\n" + memoryItem
	}
	// KNNQuery += Query

	/*** Change algorithm to The k-nearest neighbors (KNN) algorithm **/
	var resDocs interface{}
	var KNNGetErr error

	switch llm.SearchAlgorithm {
	case SimilaritySearch:
		// Retrieve related documents using cosine similarity search

		resDocs, KNNGetErr = llm.CosineSimilarity(KNNPrefix, KNNQuery, llm.RagRowCount, llm.ScoreThreshold)
	case KNearestNeighbors:
		// Retrieve related documents using KNN search
		resDocs, KNNGetErr = llm.FindKNN(KNNPrefix, KNNQuery, llm.RagRowCount, llm.ScoreThreshold)
	default:
		return nil,nil, errors.New("unknown search algorithm")
	}

	if KNNGetErr != nil {
		if !llm.AllowHallucinate {
			return nil,nil, KNNGetErr
		}
	}
	// Check if relevant documents were retrieved
	hasRag := resDocs != nil && len(resDocs.([]schema.Document)) > 0
	if !hasRag && llm.FallbackLanguage != "" && llm.FallbackLanguage != Language {

		switch llm.SearchAlgorithm {
		case SimilaritySearch:
			resDocs, KNNGetErr = llm.CosineSimilarity(prefix+":"+llm.FallbackLanguage+":", KNNQuery, llm.RagRowCount, llm.ScoreThreshold)
		case KNearestNeighbors:
			resDocs, KNNGetErr = llm.FindKNN(prefix+":"+llm.FallbackLanguage+":", KNNQuery, llm.RagRowCount, llm.ScoreThreshold)
		default:
			return nil,nil, errors.New("unknown search algorithm")
		}

		// resDocs, KNNGetErr := llm.CosineSimilarity(KNNPrefix, KNNQuery, llm.RagRowCount, llm.ScoreThreshold)
		// resDocs, KNNGetErr = llm.FindKNN(prefix+":"+llm.FallbackLanguage+":", Query, llm.RagRowCount, llm.ScoreThreshold)
		if KNNGetErr != nil {
			if !llm.AllowHallucinate {
				return nil,nil, KNNGetErr
			}
		}
		hasRag = resDocs != nil && len(resDocs.([]schema.Document)) > 0

	}

	var curMessageContent llms.MessageContent
	var ragArray []llms.ContentPart
	ragText := ""

	// Prepare the language detection function based on the query
	languageCapabilityDetectionFunction := `detect language of "` + Query + `"`
	languageCapabilityDetectionText := `detected language without mentioning it.`
	if llm.LLMModelLanguageDetectionCapability {
		languageCapabilityDetectionFunction = `{language} = detect_language("` + Query + `")`
		languageCapabilityDetectionText = "{language}"

	} else {
		if llm.AnswerLanguage != "" {
			languageCapabilityDetectionFunction = ""
			languageCapabilityDetectionText = llm.AnswerLanguage
		}
	}

	// If no relevant documents found, handle response accordingly

	if !hasRag {
		if !llm.AllowHallucinate {
			if llm.NoRagErrorMessage != "" {
				ragText = languageCapabilityDetectionFunction + `
				You are an AI assistant, Think step-by-step before answer.
				your only answer to all of questions is the improved version of "` + llm.NotRelatedAnswer + `" in ` + languageCapabilityDetectionText + `.
				Assistant:`

				msgs = append(msgs, llms.TextParts(llms.ChatMessageTypeSystem, ragText))
			} else {
				return nil,nil, errors.New("rag query has no results and hallucination is allowed but NoRagErrorMessage is empty")
			}
		}
	} else {
		for _, doc := range resDocs.([]schema.Document) {
			ragText += "\n" + doc.PageContent
		}
		ragText = languageCapabilityDetectionFunction + `
		You are an AI assistant with knowledge only and only just this text: "` + ragText + `".
		Think step-by-step and then answer briefly in ` + languageCapabilityDetectionText + `. If question is outside this scope, add "@" to the beginning of response and Just answer in ` + languageCapabilityDetectionText + ` something similar to "` + llm.NotRelatedAnswer + ` without mentioning original text or language information."
		User: "` + Query + `"
		Assistant:`
		ragArray = append(ragArray, llms.TextPart(ragText))
		curMessageContent.Parts = ragArray
		curMessageContent.Role = llms.ChatMessageTypeSystem
		msgs = append(msgs, curMessageContent)
	}

	// Include session history in the query

	QueryWithMemory := Query

	if len(mem.Questions) > 0 {
		QueryWithMemory += "Here is the context from our previous interactions:\n"
		for idx, q := range mem.Questions {
			QueryWithMemory += "\t" + strconv.Itoa(idx) + "." + q + "\n"
		}

	}

	msgs = append(msgs, llms.TextParts(llms.ChatMessageTypeHuman, QueryWithMemory))
	memoryAddAllowed := hasRag
	isFirstWord := true
	// Generate content using the LLM and stream results via the provided callback function
	response, err := llmclient.GenerateContent(ctx,
		msgs,
		llms.WithTemperature(llm.Temperature),
		llms.WithTopP(llm.TopP),
		llms.WithStreamingFunc(func(ctx context.Context, chunk []byte) error {
 
			if isFirstWord && len(chunk) > 0 {
				startsWithAt := chunk[0] == 64
				if memoryAddAllowed && startsWithAt {
					memoryAddAllowed = false
				}
				isFirstWord = isFirstWord && chunk[0] != 32
				if isFirstWord && startsWithAt {
					chunk = chunk[1:]
				}
			}
			return StreamingFunc(ctx, chunk)
		}),
	)

	// Update memory with the new query if RAG data was found
	if hasRag && memoryAddAllowed {
		if exists {
			mem.Questions = append(mem.Questions, Query)
			llm.MemoryManager.AddMemory(sessionID, mem.Questions)
		} else {
			llm.MemoryManager.AddMemory(sessionID, []string{Query})
		}
	}

	return response,resDocs, err
}
